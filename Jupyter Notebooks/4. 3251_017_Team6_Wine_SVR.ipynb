{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Prediction using Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Support vector machine (SVM) is a popular machine learning technique well suited for both classification and regression tasks. This notebook provides a brief introduction to SVM and showcases its application to wine quality prediction using UCI's Wine Quality Data Set.\n",
    "\n",
    "### 1.1 Support Vector Machine\n",
    "Consider the dataset in following image. \n",
    "\n",
    "![image1](..\\Resources\\svm_img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this image, we have got few samples belonging to two classes: one of them is represented in white circles and other in dark circles. X<sub>1</sub> and X<sub>2</sub> are two features (attributes or measurements or predictors) which are being studied for their accuracy of prediction of the two classes of interest. The task of the classifier is to find a 'boundary' between samples of the dataset such that samples get separated as well as possible. The classifiers carryout a statistical analysis on the (training) dataset and find such boundaries automatically. While finding an optimal boundary a classifier may evaluate many boundary candidates, like H<sub>1</sub>, H<sub>2</sub> and H<sub>3</sub> shown in the figure and decide on a best candidate. Note that, H<sub>1</sub> does NOT separates all samples but H<sub>2</sub> and H<sub>3</sub> do. Though both H<sub>2</sub> and H<sub>3</sub> are valid boundaries, H<sub>3</sub> is considered better since it leaves out 'maximum' space for 'unseen' samples (samples that are not in training set). SVM is formulated to efficiently find the 'maximal margin hyperplane' (the red line) for a given dataset. In other words, SVM is guaranteed to find a maximal margin hyperplane if one exists (i.e. data is linearly separable).\n",
    "\n",
    "That brings us to the question, what happens when data is NOT linearly separable. Now consider below dataset (left part of the image):\n",
    "\n",
    "![image1](..\\Resources\\svm_img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to previous dataset, this dataset too has samples belonging to two classes and their two predictors are in x and y axis. Just that, there is NO nice line separating them any more. What makes SVMs really popular is their ability to work well even in this type of datasets. SVM uses a notion of 'kernel' to map the original dataset to a very high dimensional space, where the samples become linearly separable (please see right part of the image). SVMs do this automatically for us. This kernel mapping is based on a strong mathematical concept which makes mapping to very large dimensional space computationally feasible.\n",
    "\n",
    "Support Vector Machine can be applied not only to classification problems but also to the case of regression. Still it contains all the main features that characterize maximum margin algorithm: a non-linear function is learned by linear learning machine mapping into high dimensional kernel induced feature space. The capacity of the system is controlled by parameters that do not depend on the dimensionality of feature space.\n",
    "\n",
    "In this notebook, we will be using SVR implementation provided by python's scikit learn library. The library has several in-built kernels including Radial Basis Function or RBF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "Let us start with loading the data and take a quick peek at it for sanity check. The dataset contains details of white and red wine in separate csv files. We load both of these files and combine them in a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  color  \n",
       "0      8.8        6      0  \n",
       "1      9.5        6      0  \n",
       "2     10.1        6      0  \n",
       "3      9.9        6      0  \n",
       "4      9.9        6      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "urlw='../Resources/winequality-white.csv'\n",
    "dataset_r = pd.read_csv(urlw, sep=';')  \n",
    "dataset_r[\"color\"] = 0\n",
    "urlr='../Resources/winequality-red.csv'\n",
    "dataset_w = pd.read_csv(urlr, sep=';')  \n",
    "dataset_w[\"color\"] = 1\n",
    "dataset = pd.concat([dataset_r, dataset_w], axis=0)\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset appears to have been loaded properly. We have 6497 samples in our dataset. The column 'quality' is our target column and remaining 11 columns provide various physicochemical properties of wine samples. We have added one additional column 'color' which is set 0 for red wine and 1 for white.\n",
    "\n",
    "Next, let us look at the distribution of our target column 'quality'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Test Set\n",
    "Process of building a machine learning model requires significant amount of parameter tuning before we arrive at a good model. In this process, however, we might end up over fitting to our data. Hence, at the end of this process we need to 'confirm' that we have not over fit and our model indeed works well on unseen data. All that we have to do is to keep few samples out of the training phase, call that a test set and use them only one time at the end of the training phase. Usually, performance of the model is reported on such test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns = \"quality\")\n",
    "y = dataset[\"quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples = 5847\n",
      "Number of test samples = 650\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=0)\n",
    "\n",
    "print(\"Number of train samples = \" + str(X_train.shape[0]))\n",
    "print(\"Number of test samples = \" + str(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have 5847 samples in train set and remaining about 10% in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Normalization\n",
    "When dataset columns have different value ranges, often times, the loss function tend to give 'less importance' to columns with smaller value ranges. Data normalization is a technique to alleviate this undesirable effect.\n",
    "\n",
    "First, let us confirm that the column values are having different value ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chlorides</th>\n",
       "      <th>density</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>pH</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.98711</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.72</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.611</td>\n",
       "      <td>1.03898</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.01</td>\n",
       "      <td>14.9</td>\n",
       "      <td>15.9</td>\n",
       "      <td>65.8</td>\n",
       "      <td>289.0</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     chlorides  density  citric acid  volatile acidity  sulphates    pH  \\\n",
       "min      0.009  0.98711         0.00              0.08       0.22  2.72   \n",
       "max      0.611  1.03898         1.66              1.58       2.00  4.01   \n",
       "\n",
       "     alcohol  fixed acidity  residual sugar  free sulfur dioxide  \\\n",
       "min      8.0            3.8             0.6                  1.0   \n",
       "max     14.9           15.9            65.8                289.0   \n",
       "\n",
       "     total sulfur dioxide  \n",
       "min                   6.0  \n",
       "max                 440.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[['chlorides', 'density', 'citric acid', 'volatile acidity', 'sulphates', 'pH', 'alcohol', 'fixed acidity', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide']].describe().loc[['min','max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say 'chlorides', 'density', 'citric acid', 'volatile acidity', 'sulphates' have low dynamic range. 'pH', 'alcohol', 'fixed acidity', 'residual sugar' have medium dynamic range and 'free sulfur dioxide', 'total sulfur dioxide' have very high value ranges.\n",
    "\n",
    "There are many ways one can normalize this data. We are going for most frequently used StandardScaler() which standardizes features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishali\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Vaishali\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\Vaishali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us do Grid-Search for the best model on this standardized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build and test SVR Model\n",
    "We can tune three parameters of SVR namely, \n",
    "- kernel - linear or non-linear (for example : RBF)\n",
    "- C      - Penalty term to prevent over fitting\n",
    "- gamma  - Kernel coefficient\n",
    "\n",
    "Parameters 'C' and 'gamma' can be set to any arbitrary positive floating-point numbers implying that we can try really large number of different models on the same train set. Hence, we need a mechanism to 'select' the best one of these models. That is where cross-validation comes into picture. Idea is that we divide our train set into k subsets, train the model with 'one particular combination of above parameters' on k-1 subsets and validate on the remaining subset. We do this k times leaving out different subset every time and fixing a parameter combination. At the end of k<sup>th</sup> step, we average out the performance across k steps. That will be the performance for that particular parameter combination. Now we change the parameter combination and repeat above k steps until we have tried all the parameter combination of interest. That is lot of computation. Fortunately, python utility GridSearchCV() can do that automatically for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "              {'C': [1, 2,4,6,8], 'kernel': ['rbf'],\n",
    "               'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Score: 0.36\n",
      "Best Kernel: rbf\n",
      "Best C: 2\n",
      "Best Gamma: 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_model = GridSearchCV(svm.SVR(kernel='rbf',epsilon=1.0), parameters, cv=5)\n",
    "grid_model.fit(X_train_norm, y_train)\n",
    "print('Best Model Score: {:.2f}'.format(grid_model.best_score_))\n",
    "\n",
    "\n",
    "print('Best Kernel:', grid_model.best_estimator_.kernel)\n",
    "print('Best C:', grid_model.best_estimator_.C)\n",
    "print('Best Gamma:', grid_model.best_estimator_.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'svr_model.model'\n",
    "pickle.dump(grid_model.best_estimator_, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common practice to report the model accuracy on a hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4477171108966258"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(r2_score(y_test, grid_model.predict(X_test_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried both 'linear' kernel and 'rbf'. For linear kernel, we tried 4 different penalty values 1, 10, 100 and 100. Additionally, for RBF kernel we experimented with 9 different gamma values ranging from 0.1 to 0.9. To assess a combination, we utilized cross validation with k = 5.\n",
    "\n",
    "In spite of such exhaustive Grid Search, the best model $R^2$ is just 0.44. Though this number is still small, we have achieved nearly 12% improvement when we switched from Linear Regression algorithm to Support Vector Regression which is encouraging. Maybe we can do better if we use XGBoost or Random Forest with a more refined GridSearch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
